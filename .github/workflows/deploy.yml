name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Force deployment even if tests fail'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: brs-backend
  EKS_CLUSTER_NAME: brs-cluster
  PYTHON_VERSION: '3.11'

jobs:
  test:
    runs-on: ubuntu-latest
    if: ${{ !inputs.force_deploy }}
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: brs_test
          POSTGRES_USER: test_user
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Poetry
      uses: snok/install-poetry@v1
    
    - name: Install dependencies
      run: poetry install --no-interaction --no-ansi
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/brs_test
        SECRET_KEY: test-secret-key-for-ci
        ENVIRONMENT: test
      run: |
        poetry run pytest --cov=app --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  build-and-deploy:
    runs-on: ubuntu-latest
    needs: [test]
    if: always() && (needs.test.result == 'success' || inputs.force_deploy)
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
    
    - name: Scan Docker image for vulnerabilities
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        aws ecr describe-image-scan-findings \
          --repository-name $ECR_REPOSITORY \
          --image-id imageTag=$IMAGE_TAG \
          --region $AWS_REGION || echo "Scan not completed yet"
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_REGION
    
    - name: Replace placeholders in Kubernetes manifests
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
        AWS_ACCOUNT_ID: ${{ steps.login-ecr.outputs.registry }}
      run: |
        # Create a temporary directory for processed manifests
        mkdir -p k8s-processed
        
        # Copy and process each manifest
        for file in k8s/*.yaml; do
          if [ -f "$file" ]; then
            filename=$(basename "$file")
            sed "s|<AWS_ACCOUNT_ID>|${AWS_ACCOUNT_ID%%.dkr.ecr.*}|g; \
                 s|<REGION>|${AWS_REGION}|g; \
                 s|latest|${IMAGE_TAG}|g; \
                 s|ACCOUNT_ID|${AWS_ACCOUNT_ID%%.dkr.ecr.*}|g" \
                 "$file" > "k8s-processed/$filename"
          fi
        done
    
    - name: Deploy to EKS
      run: |
        # Apply manifests in order
        kubectl apply -f k8s-processed/namespace.yaml
        kubectl apply -f k8s-processed/rbac.yaml
        kubectl apply -f k8s-processed/secrets.yaml
        kubectl apply -f k8s-processed/deployment.yaml
        kubectl apply -f k8s-processed/service.yaml
        kubectl apply -f k8s-processed/ingress.yaml
        kubectl apply -f k8s-processed/hpa.yaml
        
        # Wait for deployment to be ready
        kubectl rollout status deployment/brs-backend -n brs-production --timeout=600s
    
    - name: Verify deployment
      run: |
        # Check pod status
        kubectl get pods -n brs-production -l app=brs-backend
        
        # Check service endpoints
        kubectl get svc brs-backend-service -n brs-production
        
        # Check ingress
        kubectl get ingress brs-backend-ingress -n brs-production
        
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=brs-backend -n brs-production --timeout=300s
    
    - name: Run post-deployment health check
      run: |
        # Get the load balancer URL (this might take some time to provision)
        echo "Waiting for load balancer to be ready..."
        sleep 60
        
        # Check if ingress has an address
        kubectl get ingress brs-backend-ingress -n brs-production -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "Load balancer not ready yet"
    
    - name: Cleanup processed manifests
      run: rm -rf k8s-processed

  notify:
    runs-on: ubuntu-latest
    needs: [build-and-deploy]
    if: always()
    
    steps:
    - name: Notify deployment status
      run: |
        if [ "${{ needs.build-and-deploy.result }}" == "success" ]; then
          echo "✅ Deployment to production completed successfully!"
        else
          echo "❌ Deployment to production failed!"
          exit 1
        fi


